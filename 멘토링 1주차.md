# 멘토링 1주차

날짜: 2021년 5월 1일
속성: 멘토링
태그: 하늘 정, 재언 조, 예찬 이, 원용 이, 보라 김, 대영 전

## 오후

- 클라우드

    멘토님의 피드백

    1. **목적이 명확해야한다**
        - 적용시켜야 할 대상을 명확하게 한다
        - ~한 불편한 이슈를 해결하기 위해 이걸 만들었다 → 주제가 명확하게 잡혀야지 4가지부문(iot,ai,cloud,big data)를 적용하기가 쉬울거 같다)
        - ex) 범죄 예방을 위한 딥페이크 영상 변조확인라 큰 주제만 잡았지 명확한 대상이 없다
        - ex) 머니브레인 - 페이스포렌식(경찰쪽에서 범죄 수사할때 쓰는 기법) - > 대상이 잡혀있다
    2. 4가지부문(iot,ai,cloud,big data)이 적용 되어야 한다.
        - 우리의 지금 딥페이크 아이디어에서 iot부문이 잘 드러나 있지 않다
        - 4가지 부문이 어느정도 들어가게 요구사항을 구성해야한다.
    3. 멘토님들이 같이 생각해주신 아이디어
        - 딥페이크 영상 변조 확인이 주제가 아닌 그냥 딥페이크만드는 체험은 어떤지?
        - 요즘 유튜버나 일반 블로그등 동영상 업로드 형태가 많다 → 그럼 딥페이크 영상인지 판별하는건 어떨가? (주 대상 : 동영상을 올리는 유튜버, 블로거?)
            - 딥페이크 영상이면 업로드가 안된다
            - 딥페이크 영상이 아니면 업로드 되는 형식
        - 아니면 딥페이크 영상분석을 잘 구현해서 오픈소스 형식으로 사람들에게 제공하는 방식
        - 이 딥페이크 아이디어로 4가지를 녹여낼 수 없다면 다르게 생각해보자
            - `데이터 레이크` / `CI/CD` / `데이터 파이프라인`과 관련된 분야(빅데이터와 클라우드가 융합된 분야)

        - **즉, 기능점 관점보다는 비즈니스 관점으로 봐보자**

        4. 성공 여부

        - 꼭 성공해야 한다고 생각하지 말자
        - ~걸 시도했고 ~~걸 했다
- 빅데이터
    - 주제 방향: 나이, 인종 분류 /  안면인식, 인증 
    mvp 수준, 경량화 필수 / 연예인 이미지 타겟
    - 모델의 성능보단 완결 위주, 데이터 개수 보다 정확도?로
    - 최종 목적을 딥페이크 범죄예방으로 **주제 축소, 경량화**
    - 가상을 만들고 진짜랑 섞어서(연구라면), 전처리(기획에만?)
    - 연구기획 초안st
    - 실효성
- AI

    Q1. 모델학습용 EC2 서버와 웹서버를 분리하여 데이터처리를 할 건데, 구현시 어떻게 해야할지 감이 안잡힙니다. 어떤식으로 접근해야할지, 그리고 구현할때 인터넷에 검색한다면 어떤 키워드를 보면 될까요?

    A.

    1. 웹서버는 prediction만 수행해서 받아오는것
    2. 모델학습이 끝나고 난 데이터(weight)는 로그로 저장해서 웹서버에서 가져올수 있게. 자동화는 필요x
    3. 모델파일을 웹서버에 따로 떨굴 수 있음
    4. django웹서버 rest api(aws에서도 해줄 수 있음)사용하면 좀 더 간단

    Q2. 안드로이드 앱으로 접속하여 사용자가 request할때 웹서버를 통하는게 맞나요?

    A. 맞음.

    Q3. 클라우드, 빅데이터, AI, IoT를 접목하여 프로젝트를 할 때 아키텍처를 구체적으로 세우는것이 어려워서 인터넷에 검색하여 알아보고 싶습니다. 어떤 방식으로 키워드를 생각하면 될까요?

    ## 멘토님의 코멘트

    주제를 좀 더 구체적으로 정하여 목적을 만들기

    - 즉, 모든 분야가 참여할 수 있는 기능

    ## 주제와 목적을 정하기 위해 수행할 점

    - 학습에 필요한 데이터를 3일내로 가져올 수 있는지
    - 현재 소스코드를 가져올 수 있는지
    - 기술을 나열하고 엮어서 프로젝트 만들기!

- IoT
    - 피싱 방지
    - 앱을 통해 폰에서 음성 및 문자에서 특정 단어를 검출
    - 효과 -> (1) 보호자의 폰으로 알림, (2) 특정 센서 작동
    - 웹캠, 마이크
    - **라즈베리파이, 디바이스(센서) 무조건 사용**
    : 공익적 목적에 맞게 주제를 다시 선정
    - 딥 페이크는 한 달 안에 절대로 완성할 수 없음
    - AI 허브 -> 제한적, 양이 절대적으로 부족, 저품질, 공개 여부 문제

### **주제 구체화**

- 딥페이크랑 안면 인식
- 대상:
- 목적:
- ~~~을 활용해(기술 별) ~~를 위한 ~~~ 주제(5/3 월요일 아침 회의 전까지 작성하기)
    - AI:
        - 김보라 (IoT 기능만)
            1. 보안 기능

                라즈베리파이 파이카메라 (or 웹캠)로 사용자의 얼굴을 동영상 (or 이미지)으로 수집 (OpenCV 기술 사용)하여 딥페이크인지 아닌지 판별하여 보안인가된 사람만 접근할 수 있게 한다.

                - 딥페이크로 조작된 동영상 (or 이미지)이 아니라면, 출입가능한 데이터셋에 의해서 출입가능하게 한다
                - 오픈소스 [[https://github.com/insung3511/OpenCV_Face_detection_code](https://github.com/insung3511/OpenCV_Face_detection_code)]
                - DFD [[https://miro.com/welcomeonboard/h77Ghq2oz3umYd0J3a5aU6Y4Fjp2jWDWNllje4RgZVr3cauEhXAS0Lhn7dpa9LcS](https://miro.com/welcomeonboard/h77Ghq2oz3umYd0J3a5aU6Y4Fjp2jWDWNllje4RgZVr3cauEhXAS0Lhn7dpa9LcS)]
            2. 딥페이크 영상 만들기

                라즈베리파이 파이카메라 (or 웹캠)로 사용자의 얼굴을 동영상을 수집 (OpenCV 기술 사용)하여 딥페이크 영상을 만든다

                - 마이클잭슨을 딥페이크 통해 새 앨범 출시, 아인슈타인을 딥페이크로 수업 영상 제작
                - github의 데이터셋 + 추가할 데이터셋(옵션)
        - 이원용
            1. 보안기능( + IoT추가)
                1. 식당 온도측정카메라 처럼 평소엔 비활성화 - 동작인식 시 화면 및 카메라 켜짐(IoT)
                2. 초음파로 거리 감지 (IoT)
                3. 영상(혹은 이미지) 감지 (IoT)
                4. 딥페이크 판별
                5. 데이터베이스 비교
                6. 딥페이크 혹은 데이터베이스에 없는 경우 경보 (IoT)

                    ⇒ 딥페이크 판별 아니여도 안면인식 보안 자체만 해도 융복합 가능,,,

            2. 엔터테이먼트?
                1. IoT ....xxxx

            3. Face_mask_detection
                1. [https://www.kaggle.com/dhruvmak/face-mask-detection](https://www.kaggle.com/dhruvmak/face-mask-detection) 
                    1. 마스크 착용 판별 후 마스크 제공(iot?)
                2. 마스크 쓴 얼굴로 출퇴근 기록 + 온도(적외선발열체크) 저장

    - 빅데이터:

        (1) 딥페이크 변조 영상 판별을 위한 다양한 얼굴이미지 파일 수집

        (2)  이미지 파일에 대한 전처리

        (3) 위험성 인식 증진을 위해 딥페이크 관련 뉴스기사를 수집해서 텍스트 분석 담당

    - 클라우드

        딥페이크를 어떠한 대상으로 사용할까...(iot 접목시켜서)

        - 대상이 생각이 나지 않는다

        정 안되면 얼굴인식인데 (마스크 인식) 하는 방향으로 바꾸는건 어떨까? (난이도 조정)

        대상 

        - 일반 출입 건물에서도 사용 가능
        - 버스 (좌석에 앉아서 마스크를 벗는 사람들을 방지하고자 사용)
            - 사용방법은 좌석에 카메라를 설치(카메라 구멍만 보이고 화면은 안보이게 가린다 → 전력문제는 프리미엄 버스에는 usb를 제공하니까 그곳과 연결하여 사용하면 되지 않을까?)

        원리

        - 마스크를 벗는걸 인식을 하고 부저 센서가 울리는 형식
        - 웹이 아닌 앱으로 구성(이 부분은 iot분과 논의 해야할거 같다)

        보완해야할 부분

        - 라즈베리파이 기기연동 (지난 프로젝트에서는 기술력이 부족해서 라즈베리파이(iot) 를 쓰지 못하고 그냥 노트북에 달려잇는 캠이나 웹캠을 사용하여 프로젝트 진행)
        - 앱 연동 (지난 프로젝트에서는 unity로 간단하게 앱 만들어서 사용)

        소스코드

        - 저번에 알려줬던 그 사이트를 통해서 학습모델을 구현할 수 있다.
        - 그 학습모델의 오픈소스를 분석

        ```python
        from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
        from tensorflow.keras.preprocessing.image import img_to_array
        from tensorflow.keras.models import load_model
        from imutils.video import VideoStream
        import sounddevice as sd
        import scipy.io.wavfile
        import scipy.io as sio
        import numpy as np
        import datetime
        import imutils
        import pymssql
        import struct
        import socket
        import pickle
        import time
        import sys
        import cv2
        import os

        # 동일인물 감지를 위한 변수
        Locs_Queue = []

        # 열감지를 위한 튜너
        TEMP_TUNER = 2.25
        TEMP_TOLERENCE = 70.6
        max_temperature = 0

        # ip와 port 설정
        HOST = 'localhost'
        PORT_SQL = 9000
        PORT_FILE = 8000
        ADDR = (HOST,PORT_FILE)

        def main():
            # 컴퓨터에서 직렬화 된 얼굴 탐지기 모델로드
            print("[INFO] loading face detector model...")
            prototxtPath = os.path.sep.join(["face_detector", "deploy.prototxt"])
            weightsPath = os.path.sep.join(["face_detector",
        	    "res10_300x300_ssd_iter_140000.caffemodel"])
            faceNet = cv2.cv2.dnn.readNet(prototxtPath, weightsPath)

            # 컴퓨터에서 얼굴 마스크 탐지기 모델로드
            print("[INFO] loading face mask detector model...")
            maskNet = load_model("mask_detector.model")

            # MSSQL 서버 로드
            print("[INFO] loading Database...")
            conn = pymssql.connect(host='61.81.99.173', port=PORT_SQL, user='kjh', password='kjh', database='mask_pj')
            cursor = conn.cursor()

            # 소켓생성 및 연결
            print("[INFO] Connecting Socket...")
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.connect(ADDR)
            
            # 경고음 wav파일 로드
            samplerate, sound_data = sio.wavfile.read('sound_mask.wav')

            # VideoStream 초기화 및 카메라 센서 준비
            print("[INFO] starting video stream...")
            vs = VideoStream(src=0).start()
            time.sleep(2.0)

            # 발열온도 저장 변수
            temperature_now = 0
            
            # 발열자 기준온도 설정
            temperature_max = float(input('maximum temperature : '))
            
            # 동일인물 감지를 위한 변수 선언
            isDetected = False
            ReadyForAlert = True

            while True:
                # 마스크 착용여부 확인(처음엔 적당한 거리에 탐지될때까지 실행)
                while(len(Locs_Queue) <= 2):
                    # 한프레임 촬영
                    frame = vs.read()
                    frame = imutils.resize(frame,width=600,height=400)

                    # 감지여부 및 얼굴위치, 마스크 착용여부 리턴
                    isDetected, (locs, preds) = Predict_And_Detect_Mask(frame, faceNet, maskNet)
                    # 감지가 될 경우 얼굴위치를 큐에 삽입
                    if(isDetected == True):
                        Locs_Queue.append(locs)
                    # 감지가 안되었을 경우 초록선 위에 서달라는 메세지 표시
                    else:
                        background = np.zeros((450,600,3),np.uint8)
                        point = 130,225
                        cv2.putText(background,'stand on green line',
                                    point,cv2.FONT_HERSHEY_DUPLEX,
                                    1,(255,255,255),2,cv2.LINE_AA)
                        cv2.imshow("Frame", background)
                        key = cv2.cv2.waitKey(1) & 0xFF
                        if key == ord("q"):
                            sock.close()
                            cv2.destroyAllWindows()
                            return

                # 같은 사람이 거리 내에 감지될 경우
                if(isDetected == True and Predict_Same_Person(locs) == True):
                    # 열감지를 실시합니다
                    temperature_now = Measure_Temperature(frame, locs)
                    
                    if(ReadyForAlert == True):
                        nowtime = time.strftime('%Y-%m-%d %H:%M:%S')
                        # 처음 본 사람인 경우, 마스크 착용여부 알림 발생
                        Alert_Mask(preds, locs, samplerate, sound_data)
                        # db에 새로운 사람을 추가합니다.
                        print('['+str(int(temperature_now))+'C] Welcome')
                        cursor.execute('insert into person(temp,time) values(%s, %s);',
                                       (int(temperature_now), nowtime))
                        conn.commit()
                        # 기준온도 이상일 경우, 발열 이상자 DB에 저장합니다
                        if(temperature_now > temperature_max):
                            # number값 가져오기
                            cursor.execute('select max(number) from person')
                            number = int(cursor.fetchone()[0])
                            # 발열 이상자 저장
                            print(str(int(temperature_now)) + 'C')
                            passed = input('출입여부 확인 [y/n]')
                            cursor.execute('insert into st_person values(%d, %s, %s);',
                                           (number, str(number) + '.jpg', 1 if passed == 'y' else 0))
                            conn.commit()
                            # 발열자 사진 전송
                            data = pickle.dumps(frame)
                            sock.sendall(struct.pack("d", number) + struct.pack("d", len(data)) + data)
                            print('[' + str(number) + '.jpg] saved')
                        ReadyForAlert = False
                        
                    # 가져온 출력프레임 속 머리위에 마스크 착용여부 표시
                    frame = Frame_Decorate(frame, locs, preds, temperature_now)

                # 다른 사람이 감지될 경우
                if(isDetected == True and Predict_Same_Person(locs) == False):
                    ReadyForAlert = True

                # 꾸며진 출력프레임을 화면에 표시합니다
                cv2.cv2.imshow("Frame", frame)
                key = cv2.cv2.waitKey(1) & 0xFF

                #사용한 프레임의 위치정보는 제거
                Locs_Queue.pop(0)
                
                # 'q'키 입력시 종료
                if key == ord("q"):
                    break

            sock.close()
            cv2.destroyAllWindows()
            return

        def process_face(frame):
            frame = ~frame
            heatmap = cv2.applyColorMap(frame, cv2.COLORMAP_HOT)
            
            heatmap_gray = cv2.cvtColor(heatmap, cv2.COLOR_RGB2GRAY)
            ret, binary_thresh = cv2.threshold(heatmap_gray, 200, 255, cv2.THRESH_BINARY)
            
            kernel = np.ones((5, 5), np.uint8)
            image_erosion = cv2.erode(binary_thresh, kernel, iterations=1)
            image_opening = cv2.dilate(image_erosion, kernel, iterations=1)

            image_with_rectangles = np.copy(heatmap)
            
            return image_with_rectangles

        def convert_to_celsius(pixel_avg):
            # 화씨 및 섭씨온도 변환
            f = pixel_avg / TEMP_TUNER
            c = (f - 32) * 5/9
            
            return c

        def Measure_Temperature(frame,locs):
            output = process_face(frame)
            for (box,) in zip(locs,):
                # 얼굴 위치정보를 압축해제 합니다.
                (startX, startY, endX, endY) = box
                roi = output[startY:endY, startX:endX]
                roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)

                # 얼굴위치 픽셀 온도의 평균값을 가져옵니다
                mean = convert_to_celsius(np.mean(roi_gray))

                # Colors for rectangles and textmin_area
                temperature = round(mean, 2)

            return temperature

        def Predict_Same_Person(locs):
            for (box_prev, box_now) in zip(Locs_Queue[0], locs):
                (startX_prev, startY_prev, endX_prev, endY_prev) = box_prev
                (startX_now, startY_now, endX_now, endY_now) = box_now

                if ((startX_prev - startX_now)**2 +
                    (startY_prev - startY_now)**2 +
                    (endX_prev - endX_now)**2 +
                    (endY_prev - endY_now)**2) < 10000:
                    return True
                else:
                    return False

        def Alert_Mask(preds, locs, samplerate, sound_data):
            for (box, pred) in zip(locs, preds):
                # 얼굴 위치정보 및 예측모델을 압축해제 합니다.
                (startX, startY, endX, endY) = box
                (mask, withoutMask) = pred
                
                #마스크 미착용시 경보음을 재생합니다
                if(mask < withoutMask):
                    sd.play(sound_data, samplerate)
            return

        def Frame_Decorate(frame, locs, preds, temperature_now):
            for (box, pred) in zip(locs, preds):
                # 얼굴 위치정보 및 예측모델을 압축해제 합니다.
                (startX, startY, endX, endY) = box
                (mask, withoutMask) = pred

                # 얼굴주위와 텍스트를 그리는 데 사용할 클래스 레이블과 색상을 결정합니다.
                label = "Mask On" if mask > withoutMask else "No Mask"
                color = (0, 255, 0) if label == "Mask On" else (0, 0, 255)

                # 라벨에 확률을 포함합니다
                label = ("{}  {:.1f}C".format(label, temperature_now,1))

                # 출력 프레임에 라벨 및 얼굴 주위 사각형을 표시합니다
                cv2.cv2.putText(frame, label, (startX, startY - 10),
                    cv2.cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)
                cv2.cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)

            return frame

        def Predict_And_Detect_Mask(frame, faceNet, maskNet):
            # 프레임의 치수를 잡고 그로부터 blob(큰 Binary객체)을 만듭니다
            (h, w) = frame.shape[:2]
            blob = cv2.cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0))

            # 네트워크를 통해 blob객체를 전송하고, 얼굴감지기를 얻어옵니다.
            faceNet.setInput(blob)
            detections = faceNet.forward()

            # 얼굴 목록, 해당 위치 및 얼굴 마스크 네트워크의 예측 목록을 초기화합니다.
            faces = []
            locs = []
            preds = []

            # 탐지를 반복
            for i in range(0, detections.shape[2]):
        	# 탐지와 관련된 신뢰도 (확률) 추출
                confidence = detections[0, 0, i, 2]

        	# 신뢰도가 0.5 이상인지 확인하여 취약한 탐지를 필터링
                if confidence > 0.5:
        	    # 얼굴 주위 경계 상자의 (x, y) 좌표 계산
                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                    (startX, startY, endX, endY) = box.astype("int")

        	    # 사각 상자가 프레임의 치수 내에 들어가도록 설정
                    (startX, startY) = (max(0, startX), max(0, startY))
                    (endX, endY) = (min(w - 1, endX), min(h - 1, endY))

        	    # 얼굴 ROI 추출
        	    # BGR에서 RGB 채널 순서로 변환
        	    # 224x224로 크기 조정 및 전처리
                    face = frame[startY:endY, startX:endX]
                    face = cv2.cv2.cvtColor(face, cv2.cv2.COLOR_BGR2RGB)
                    face = cv2.cv2.resize(face, (224, 224))
                    face = img_to_array(face)
                    face = preprocess_input(face)
                    face = np.expand_dims(face, axis=0)
                    
                    if((endX-startX)*(endY-startY) < 8000 or
                       (endX-startX)*(endY-startY) > 26000):
                        return False, (0,0);

        	    # 면과 경계 상자를 해당 목록에 추가
                    faces.append(face)
                    locs.append((startX, startY, endX, endY))

            # 하나 이상의 얼굴이 감지 된 경우에만 예측
            if len(faces) > 0:
                preds = maskNet.predict(faces)

            # 얼굴 위치와 경계 상자 위치를 두개의 튜플로 반환
            return True, (locs, preds)

        if __name__=='__main__':
            main()
        ```

        ![%E1%84%86%E1%85%A6%E1%86%AB%E1%84%90%E1%85%A9%E1%84%85%E1%85%B5%E1%86%BC%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2004190f9be8754f9489d40ce010b8e5ae/Untitled.png](%E1%84%86%E1%85%A6%E1%86%AB%E1%84%90%E1%85%A9%E1%84%85%E1%85%B5%E1%86%BC%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2004190f9be8754f9489d40ce010b8e5ae/Untitled.png)

    - 조사한 내용

        [[python] 쉽고 간단하게 마스크 착용 유무 판별기 만들기](https://bskyvision.com/1082)

    - IoT:

`디바이스 → 라즈베리파이`  

### 라즈베리파이에서 쓰일 수 있는 기술(필수: 라즈베리&센서)

- 센서 : 초음파 센서, 부저, LED 등
- 아두이노 & 라즈베리 파이 : 시리얼 통신, mqtt 통신, pi 카메라 영상

![%E1%84%86%E1%85%A6%E1%86%AB%E1%84%90%E1%85%A9%E1%84%85%E1%85%B5%E1%86%BC%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2004190f9be8754f9489d40ce010b8e5ae/Untitled%201.png](%E1%84%86%E1%85%A6%E1%86%AB%E1%84%90%E1%85%A9%E1%84%85%E1%85%B5%E1%86%BC%201%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%2004190f9be8754f9489d40ce010b8e5ae/Untitled%201.png)

- 기획 초안

    (1) 상황 1: 라즈베리파이 내 파이썬 파일을 실행 → 각종 센서 제어로 이용자의 행위를 통제

    - 지문인식 센서 → 아두이노로 연결, 시리얼 통신으로 라즈베리파이로 상태변수를 전달
    - 초음파 센서 → 이용자의 거리 상태를 측정하고 특정 기준점을 기준으로 각 상황에 따른 여러 기기를 제어
    - 파이카메라로 촬영한 영상을 JSON 형식으로 MQTT BROKER로 전달 → MQTT SUBCRIBER로 DJango Web Sever에 입력

    (2) 상황 2: 파이카메라로 촬영한 영상을 JSON 형식으로 MQTT BROKER로 전달한 후에 분석 결과를 Django Web Server에서 가져와서 이 상태변수에 관한 여러 기기를 제어

    - 부저, LED 등으로 상태 값을 나타내는 여러 이벤트를 구현

    (3) 고려사항

    - 파이 카메라 대신 웹캠 사용 여부
    - 위의 기능을 모두 구현한 후에 안드로이드 앱을 통해 보안등급 조정기능으로 여러 기기를 추가로 제어