# 아이디어 회의

날짜: 2021년 5월 3일
속성: 아이디어
태그: 하늘 정, 재언 조, 예찬 이, 원용 이, 보라 김, 대영 전

### 대상:

### 목표:

하늘님

- 스마트 미러
    - [https://github.com/vivalahm/Smart-Mirror-FaceDetect](https://github.com/vivalahm/Smart-Mirror-FaceDetect)
    - [https://github.com/evancohen/smart-mirror](https://github.com/evancohen/smart-mirror)
    - [https://nicebam.tistory.com/4](https://nicebam.tistory.com/4)
    - [https://jeongchul.tistory.com/482](https://jeongchul.tistory.com/482)
    - [https://ebbnflow.tistory.com/242](https://ebbnflow.tistory.com/242)
    - 마스크 착용 유무, 오늘(일주일) 코로나 확진자 수 추이, 날씨, 미세먼지 현황, 관련 뉴스 등

    [2020 IT정보공학과 설계작품경진대회(스마트미러).pdf](%E1%84%8B%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%83%E1%85%B5%E1%84%8B%E1%85%A5%20%E1%84%92%E1%85%AC%E1%84%8B%E1%85%B4%204de51b3250904195ab15acdef539ad37/2020_IT_().pdf)

- 딥페이크
    - [https://www.yna.co.kr/view/AKR20191226063600002](https://www.yna.co.kr/view/AKR20191226063600002)
    - 타켓: 은행?
    - AI 경진대회 데이터

    [test.zip](%E1%84%8B%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%83%E1%85%B5%E1%84%8B%E1%85%A5%20%E1%84%92%E1%85%AC%E1%84%8B%E1%85%B4%204de51b3250904195ab15acdef539ad37/test.zip)

    - AI와 IoT 융합 > VR/AR??
    - 더빙?

재언님

- 여러 기능 추가
    - 소주제 : 출입자의 기초 신체 정보와 거주 정보 등을 바탕으로 한 자가격리 판단 기능(AI - 판별분석 or 신경망 모델?)
    - [1 단계] : 영상 촬영을 하는 과정에서 주위의 온습도, 비접촉 체온, 심장맥박 데이터를 센서로 수집
    - 센서 데이터 수집 및 mqtt 통신 관련 코드

        ```python
        import serial
        import paho.mqtt.client as mqtt
        import json
        import time
        import RPi.GPIO as GPIO
        import Adafruit_DHT as dht11
        import spidev

        ## 아두이노 시리얼 통신 설정
        port = "/dev/ttyACM0"
        brate = 9600
        seri = serial.Serial(port, baudrate=brate, timeout=None)

        ## Pin 번호 설정
        DHT = 17
        servopin = 23

        GPIO.setmode(GPIO.BCM)
        GPIO.setup(servopin, GPIO.OUT)

        pwm = GPIO.PWM(servopin, 50)
        pwm.start(0)
        time.sleep(0.2)

        # MCP3008 채널 중 센서에 연결한 번호 설정 : PIN CHANNEL Number
        GAS = 0

        # SPI 통신을 하기 위한 객체 생성
        spi = spidev.SpiDev()

        # SPI 통신 시작하기
        spi.open(0, 0)

        # SPI 통신 속도 설정
        spi.max_speed_hz = 500000

        def readAdc(adcNum):
            if adcNum < 0 or adcNum > 7:
                return -1

            # MCP3208과 통신하기 위한 패킷을 설정하는 작업
            r = spi.xfer2([1, 8 + adcNum << 4, 0])

            # SPI를 통신을 통해서 받아온 센서 데이터는 8bit짜리 이고 이를 16bit로 통합하는 과정
            data = ((r[1] & 3) << 8) + r[2]
            return data

        ## mqtt 통신(publisher) 설정
        client = mqtt.Client()

        client.connect("172.30.1.34", 1883, 60)

        try:
            while True:
                dust = seri.readline()
                dust = float(dust.decode()[:-2])
                if dust <= 0:
                    pass
                hum, temp = dht11.read_retry(dht11.DHT11, DHT)
                # readadc 함수로 pot_channel의 SPI 데이터를 읽기
                gas_value = readAdc(GAS)
                value = {"hum": hum, "temp": temp, "gas": gas_value,"dust": dust}

                client.publish("adminCustom", json.dumps(value))

                if hum is not None and temp is not None:
                    print("hum : " + str(hum) + ", temp : " + str(temp))
                    if temp == 20:
                        pwm.ChangeDutyCycle(3)
                        time.sleep(1)
                    elif temp == 22:
                        pwm.ChangeDutyCycle(6)
                        time.sleep(1)
                    elif temp == 24:
                        pwm.ChangeDutyCycle(9)
                        time.sleep(1)
                    else:
                        pwm.ChangeDutyCycle(12)
                        time.sleep(1)
                else:
                    print("error")

        except KeyboardInterrupt:
            GPIO.cleanup()

        finally:
            pwm.stop()
            GPIO.cleanup()
        ```

    - [2 단계] : 코로나19 관련 질병관리청 공공 API에서 웹 스크롤링으로 분석 데이터
        - 공공API [[https://www.data.go.kr/index.do](https://www.data.go.kr/index.do)]

원용님

- 딥페이크 보안기능( + IoT)
    1. 식당 온도측정카메라 처럼 평소엔 비활성화 - 동작인식 시 화면 및 카메라 켜짐(IoT)
    2. 초음파로 거리 감지 (IoT)
    3. 영상(혹은 이미지) 감지 (IoT 아니면 웹캠)
    4. (딥페이크 판별)
    5. 데이터베이스 비교
        1. [http://cmusatyalab.github.io/openface/](http://cmusatyalab.github.io/openface/) 
        2. [https://wiserloner.tistory.com/1123](https://wiserloner.tistory.com/1123) 
    6. 딥페이크 혹은 데이터베이스에 없는 경우 경보 (IoT)

    - **Face_mask_detection**
        1. 마스크 쓴 얼굴로 출퇴근 기록(DB/클라우드) + 온도(적외선발열체크/IoT) 저장
            1. 탐지모델 (키포인트 사용)
            2. 
        2. 마스크 착용 판별 후 없으면 마스크 제공 (간단한 IoT) 
            1. [https://www.pyimagesearch.com/2020/05/04/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/](https://www.pyimagesearch.com/2020/05/04/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/) 
            2. [https://www.kaggle.com/dhruvmak/face-mask-detection](https://www.kaggle.com/dhruvmak/face-mask-detection)  

보라님

- 딥페이크 (+ IoT 기능)
    1. 보안 기능

        라즈베리파이 파이카메라 (or 웹캠)로 사용자의 얼굴을 동영상 (or 이미지)으로 수집 (OpenCV 기술 사용)하여 딥페이크인지 아닌지 판별하여 보안인가된 사람만 접근할 수 있게 한다.

        - 딥페이크로 조작된 동영상 (or 이미지)이 아니라면, 출입가능한 데이터셋에 의해서 출입가능하게 한다
        - 오픈소스 [[https://github.com/insung3511/OpenCV_Face_detection_code](https://github.com/insung3511/OpenCV_Face_detection_code)]
        - DFD [[https://miro.com/welcomeonboard/h77Ghq2oz3umYd0J3a5aU6Y4Fjp2jWDWNllje4RgZVr3cauEhXAS0Lhn7dpa9LcS](https://miro.com/welcomeonboard/h77Ghq2oz3umYd0J3a5aU6Y4Fjp2jWDWNllje4RgZVr3cauEhXAS0Lhn7dpa9LcS)]
    2. 딥페이크 영상 만들기

        라즈베리파이 파이카메라 (or 웹캠)로 사용자의 얼굴을 동영상을 수집 (OpenCV 기술 사용)하여 딥페이크 영상을 만든다

        - github의 데이터셋 + 추가할 데이터셋(옵션)
- 딥페이크에서 필요한 이미지 처리 모델
    - Tensorflow.keras 모델
        - 이미지 분류
        - 뉴럴네트워크 (이미지 인식)
        - 코드

            ```python
            model = tensorflow.keras.models.load_model('keras_model.h5')
            ```

    - openCV 모델
        - 이미지 프로세싱
        - 학습된 뉴럴 네트워크를 사용하여 웹캠에 비춘 이미지를 인식
    - mtcnn 모델
        - 얼굴 검출 모델
        - 코드

            Reference [[https://github.com/ipazc/mtcnn/blob/master/example.py](https://github.com/ipazc/mtcnn/blob/master/example.py)]

            ```python
            import cv2
            from mtcnn import MTCNN

            detector = MTCNN()

            image = cv2.cvtColor(cv2.imread("ivan.jpg"), cv2.COLOR_BGR2RGB)
            result = detector.detect_faces(image)

            # Result is an array with all the bounding boxes detected. We know that for 'ivan.jpg' there is only one.
            bounding_box = result[0]['box']
            keypoints = result[0]['keypoints']

            cv2.rectangle(image,
                          (bounding_box[0], bounding_box[1]),
                          (bounding_box[0]+bounding_box[2], bounding_box[1] + bounding_box[3]),
                          (0,155,255),
                          2)

            cv2.circle(image,(keypoints['left_eye']), 2, (0,155,255), 2)
            cv2.circle(image,(keypoints['right_eye']), 2, (0,155,255), 2)
            cv2.circle(image,(keypoints['nose']), 2, (0,155,255), 2)
            cv2.circle(image,(keypoints['mouth_left']), 2, (0,155,255), 2)
            cv2.circle(image,(keypoints['mouth_right']), 2, (0,155,255), 2)

            cv2.imwrite("ivan_drawn.jpg", cv2.cvtColor(image, cv2.COLOR_RGB2BGR))

            print(result)
            ```

    - blazeface 모델 (pytorch)
        - 얼굴 검출 모델
        - 코드

            Reference [[https://github.com/hollance/BlazeFace-PyTorch/blob/master/blazeface.py](https://github.com/hollance/BlazeFace-PyTorch/blob/master/blazeface.py)]

            ```python
            import numpy as np
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            ...(생략)
            class BlazeFace(nn.Module):
                def __init__(self, back_model=False):
                    super(BlazeFace, self).__init__()

                    # These are the settings from the MediaPipe example graphs
                    # mediapipe/graphs/face_detection/face_detection_mobile_gpu.pbtxt
                    # and mediapipe/graphs/face_detection/face_detection_back_mobile_gpu.pbtxt
                    self.num_classes = 1
                    self.num_anchors = 896
                    self.num_coords = 16
                    self.score_clipping_thresh = 100.0
                    self.back_model = back_model
                    if back_model:
                        self.x_scale = 256.0
                        self.y_scale = 256.0
                        self.h_scale = 256.0
                        self.w_scale = 256.0
                        self.min_score_thresh = 0.65
                    else:
                        self.x_scale = 128.0
                        self.y_scale = 128.0
                        self.h_scale = 128.0
                        self.w_scale = 128.0
                        self.min_score_thresh = 0.75
                    self.min_suppression_threshold = 0.3

                    self._define_layers()

                def _define_layers(self):
                    if self.back_model:
                        self.backbone = nn.Sequential(
                            nn.Conv2d(in_channels=3, out_channels=24, kernel_size=5, stride=2, padding=0, bias=True),
                            nn.ReLU(inplace=True),

                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24, stride=2),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 24),
                            BlazeBlock(24, 48, stride=2),
                            BlazeBlock(48, 48),
                            BlazeBlock(48, 48),
                            BlazeBlock(48, 48),
                            BlazeBlock(48, 48),
                            BlazeBlock(48, 48),
                            BlazeBlock(48, 48),
                            BlazeBlock(48, 48),
                            BlazeBlock(48, 96, stride=2),
                            BlazeBlock(96, 96),
                            BlazeBlock(96, 96),
                            BlazeBlock(96, 96),
                            BlazeBlock(96, 96),
                            BlazeBlock(96, 96),
                            BlazeBlock(96, 96),
                            BlazeBlock(96, 96),
                        )
                        self.final = FinalBlazeBlock(96)
                        self.classifier_8 = nn.Conv2d(96, 2, 1, bias=True)
                        self.classifier_16 = nn.Conv2d(96, 6, 1, bias=True)

                        self.regressor_8 = nn.Conv2d(96, 32, 1, bias=True)
                        self.regressor_16 = nn.Conv2d(96, 96, 1, bias=True)
                    else:
                        self.backbone1 = nn.Sequential(
                            nn.Conv2d(in_channels=3, out_channels=24, kernel_size=5, stride=2, padding=0, bias=True),
                            nn.ReLU(inplace=True),

                            BlazeBlock(24, 24),
                            BlazeBlock(24, 28),
                            BlazeBlock(28, 32, stride=2),
                            BlazeBlock(32, 36),
                            BlazeBlock(36, 42),
                            BlazeBlock(42, 48, stride=2),
                            BlazeBlock(48, 56),
                            BlazeBlock(56, 64),
                            BlazeBlock(64, 72),
                            BlazeBlock(72, 80),
                            BlazeBlock(80, 88),
                        )

                        self.backbone2 = nn.Sequential(
                            BlazeBlock(88, 96, stride=2),
                            BlazeBlock(96, 96),
                            BlazeBlock(96, 96),
                            BlazeBlock(96, 96),
                            BlazeBlock(96, 96),
                        )
                        self.classifier_8 = nn.Conv2d(88, 2, 1, bias=True)
                        self.classifier_16 = nn.Conv2d(96, 6, 1, bias=True)

                        self.regressor_8 = nn.Conv2d(88, 32, 1, bias=True)
                        self.regressor_16 = nn.Conv2d(96, 96, 1, bias=True)
            ...(생략)
            ```

    # **성능**

    검출 시간 : (빠름) opencv < mtcnn (느림)

    Reference [[https://seongkyun.github.io/study/2019/03/25/face_detection/](https://seongkyun.github.io/study/2019/03/25/face_detection/)]

대영님

- 의견

    ## 대상

    - 일반 출입 건물에서도 사용 가능
    - 버스 (좌석에 앉아서 마스크를 벗는 사람들을 방지하고자 사용)
        - 사용방법은 좌석에 카메라를 설치(카메라 구멍만 보이고 화면은 안보이게 가린다 → 전력문제는 프리미엄 버스에는 usb를 제공하니까 그곳과 연결하여 사용하면 되지 않을까

    ## 기술적

    - 일반 마스크 착용 유무가 아닌 좀 더 정교하게 (턱 마스크, 코만 뺀 마스크) 구성한다. - 좀 더 세분화된 라벨링 작업
    - 무게센서로 자리에 착석했을 시 값을 전송하고 그때부터 캠이 작동할 수 있도록 구성(iot도 적용할 수 있다.) (단 엄청 작은 무게가 측정될시에도 작동되면 안되니까 조건을 건다)

- 참고링크

    [dldudcks1779/Mask-Detector](https://github.com/dldudcks1779/Mask-Detector)

    [[python] 쉽고 간단하게 마스크 착용 유무 판별기 만들기](https://bskyvision.com/1082)